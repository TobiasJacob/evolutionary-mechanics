\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{color}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}

\title{Evolutionary Mechanics}
\author{Tobias Jacob, Raffaele Guillera, Ali Muddasar}

\begin{document}

\maketitle

\begin{abstract}
    We developed an application that is able to develop mechanical structures using an evolutionary algorithm. This approach can be scaled efficiently across many different nodes.
\end{abstract}

\section{Method}

Our project is divided in two sections and corresponding layers of parallelism. The first one is solving the mechanical equations to check if a mechanical structure can withstand a force. Figure~\ref{fig:Mechanical_Simulation} shows the result of such a simulation. The shape of a figure is approximated through squares. We used \texttt{OpenMP} for this part.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, trim={0pt 0pt 0pt 5em}, clip]{images/MechaincalStructure.png}
    \caption{Result of a mechanical simulation}
    \label{fig:Mechanical_Simulation}
\end{figure}


The second stage is the evolutionary algorithm. The best 10\% survive in each round. The structures mutate and are simulated again. We used \texttt{MPI} for this part.

\subsection{Speeding up the equation solver}

\begin{figure}[p]
    \centering
    \include{graphs/mechanicsSolverResults}
    \caption{Speedup of the equation solver for different proplem sizes $N$.}
    \label{fig:Speedup}
\end{figure}

\begin{table}[p]
    \centering
    \include{tables/executionSpeedTable}
    \caption{Execution time for the equation solver for different field sizes $N$ and cores $C$}
    \label{tab:Speedup}
\end{table}

The reason for usign \texttt{OpenMP} is the tight coupeling between the data in the sparse matrix multiplication. \texttt{gprof} revealed that the equation solver spends most of it's time in the sparse matrix multiplication, however the whole equation solver class works parallelized.

\begin{itemize}
    \item \textbf{Threads are spawned at the highest level} of the \texttt{PerformanceEvaluator} class. Subsequent calls to the equation setup, equation solver or linear algrabra operations will not spawn new threads. They require the threads to be set up already, and process only their their chunk using the \texttt{omp for} directive.
    \item The \textbf{equation setup} process adds the local element striffness matrix into the global equation system. Depending on the mechanical structure, the position of different planes appears rather random. Therefore, a lock is needed to prevent a race condition. Because the matrix is sparse and each thread works on its own plane, that typically are not directly connected, it is unprobable that two threads operate on the same equation row at the same time. A global lock would introduce an unnecessary penalty, therefore each row uses it's own \textbf{lock}.
    \item All linear algebra operations of the \textbf{equation solver} work in parallel. There are two types of these operations. For addition, scalar multiplication, matrix multiplication or assigning a constant value, each thread processes its \textbf{own chunk of rows}. These operations do not require an implicit or explicit barrier. If for example, a vector addtion follows a scalar multiplication, it is fine if the first thread begins with the scalar multiplication before the second thread has finished the vector addition, since each of them operates on its own set of rows.
    
    However, in the conjugate gradient method there are also operations like the scalar product or the norm of the vector. These operations require a \textbf{reduction} and have an implicit barrier.
    \item In the beginning, a unique index has to be assigned to each Plane and corner. This is not parallelizable, as the total number of planes and corners is unkown and does not follow a predictable pattern.
\end{itemize}

The time complexity of the sequential equation solver is dominated by the sparse matrix multiplication, having a complexity of 

\begin{equation}
    O_\mathit{seqSolve}(N) = O(N^3) 
\end{equation}

as explained in the progress report. The sparse matrix multiplication is fully parallel. The indexing of the equation requires $O(N^2)$. Leaving the overhead for thread creation aside, the runtime complexity is 

\begin{equation}
    O_\mathit{parSolve}(N, C) = O \left(\frac{N^3}{C}\right) + O(N^2)
\end{equation}

for a sufficiently large $N$. The efficency is

\begin{equation}
    E = \frac{O_\mathit{seqSolve}}{O_\mathit{parSolve} C} = O\left(\frac{N^3}{N^3 + C^2}\right)
\end{equation}

The equation solver has weak scalability, because efficency remains only at the same level if $N \propto C$. In practice however, there is a significant overhead in creating the threads. The speedup becomes only notable for problem sizes $N > 50$.

The Equation Solver is still a powerful solver. It is able to solve a mechanical structure with 80600 equations in \SI{2.732}{\second} using 28 cores on bridges. It also showcases OpenMP and locks well. The speedup of the execution time of table~\ref{fig:Speedup} are shown in figure~\ref{tab:Speedup}.

\subsection{Speeding up the evolutionary algorithm}

Evolutionary algoirthms are very parallel by nature. Each organism can be evaluated independent. Then they have to be sorted. We do the sorting on a node 0. \texttt{Gather} is used in that process. The best 10\% get redistributed using \texttt{Bcast}. This is visualized in algorithm~\ref{alg:Evolution}.

A MPI Message for an evaluated board consisits of

\begin{itemize}
    \item 1 float for the score
    \item $R \times C$ bytes, each containing the boolean value of the field.
\end{itemize}

We typically simulate grid sizes of 20, meaning the message has a size of 404 bytes. Since all processes will be initialized with the same field size it has not to be stored in the message. The message size grows with 

\begin{equation}
    O_{msgSize} = O(N^2)
\end{equation}

The runtime complexity of the single thread evolutionary algorithm is

\begin{equation}
    O_\mathit{evolSeq}(N, G, A) = O_\textit{seqSolve} \cdot O(G A)
\end{equation}

with generations size $G$ and $A$ epochs. The parallel version has

\begin{equation}
    O_\mathit{evolPar}(N, C, G, A) = O \left( \frac{N^3 G A}{C} + N^2 G A\right)
\end{equation}

with $O (\frac{N^3 G A}{C} )$ being the work for solving the equation system per core and $O(N^2 G A)$ being the communcation cost for sending $O(G)$ bytes of data over the network each epoch~$A$. The efficency is

\begin{equation}
    E = O(\frac{N^3 + N^2}{N^3+C N^2})
\end{equation}

so this algorithm has also a weak scalability. The execution times for 


The result of the finished evolution is shown on figure~\ref{fig:Evolution}.


\begin{algorithm}[p]
    \caption{Evolute on node}
    \begin{algorithmic}
        \STATE Initialize all local fields fully set
        \FOR{all epochs}
            \STATE Mutate the fields
            \STATE Evaluate the fields
            \STATE \texttt{Gather} all fields into the master
            \STATE Sort the fields on the master
            \STATE \texttt{Broadcast} the best 10\% to everyone
            \STATE Replace local fields
        \ENDFOR
    \end{algorithmic}
    \label{alg:Evolution}
\end{algorithm}

\begin{table}[p]
    \centering
    \begin{tabular}{lrrrrrr}
        \toprule
        Cores & 7 & 14 & 28 & 56 & 112 & 224 \\
        Cores per Task & 1 & 1 & 1 & 1 & 1 & 2 \\
        Tasks & 7 & 14 & 28 & 56 & 112 & 112 \\
        Nodes & 1 & 1 & 1 & 2 & 4 & 8 \\
        \midrule
        N = 10 & 33.667     & 18.519 & 9,399 & 5.432 & 2.950 & 4.256 \\
        N = 20 & 296.997    & 164.875 & 91.958 & 43.114 & 25.199 & 29.429 \\
        N = 40 & (2686.816) & (1343.408) & (671.704) & 335.852 & 200.529 & 164.243 \\
        N = 80 & - & - & - & - & - & 2163.535 \\
        \bottomrule
    \end{tabular}
    \caption{Execution time in seconds for the evoultion for different field sizes $N$ and cores $C$. The generation size is set fixed to 112 and 1000 epochs are simulated.}
    \label{tab:Speedup}
\end{table}

\begin{figure}[p]
    \centering
    \include{graphs/evolutionResults}
    \caption{Speedup of the equation solver for different proplem sizes $N$.}
    \label{fig:Speedup}
\end{figure}


\clearpage

\begin{figure}[p]
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug0.png} 
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug900.png} 
    \end{subfigure}

    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug1500.png} 
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug8625.png} 
    \end{subfigure}
    \caption{Result of a evoultion, using $20\times20$ gird, 100 organisms per epoch, 1000 epochs and a alterations decay of 0.995. Note, how the structure gets wider on the top to deal with the increased bending stress.}
    \label{fig:Evolution}
\end{figure}


\end{document}