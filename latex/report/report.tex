\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{color}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}

\title{Evolutionary Mechanics}
\author{Tobias Jacob, Raffaele Guillera, Ali Muddasar}

\begin{document}

\maketitle

\begin{abstract}
    We developed an application that is able to develop mechanical structures using an evolutionary algorithm. This approach can be scaled efficiently across many different nodes.
\end{abstract}

\section{Introduction}

With increasing computing power available, a new approach is becomeing more relevant in engineering: Evolutionary algorithms. Instead of developing a structure for a problem, an engineer just specifies the structure of the problem and lets the computer come up with a solution. The hope is that the computer will come up with a better solution the engineer did not think of.

\section{Method}

Our project is divided in two sections and corresponding layers of parallelism. The first one is solving the mechanical equations to check if a mechanical structure can withstand a force. Figure~\ref{fig:Mechanical_Simulation} shows the result of such a simulation. The shape of a figure is approximated through squares. A detailed introduction on how to use the fininte element method can be found at~\cite{Nikishkov2004}. We used \texttt{OpenMP} for this part.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, trim={0pt 0pt 0pt 5em}, clip]{images/MechaincalStructure.png}
    \caption{Result of a mechanical simulation}
    \label{fig:Mechanical_Simulation}
\end{figure}


The second stage is the evolutionary algorithm. The best 10\% survive in each round. The structures mutate and are simulated again. We used \texttt{MPI} for this part.

\subsection{Speeding up the equation solver}

\begin{figure}[p]
    \centering
    \include{graphs/mechanicsSolverResults}
    \caption{Speedup of the equation solver for different proplem sizes $N$.}
    \label{fig:SpeedupSimulation}
\end{figure}

\begin{table}[p]
    \centering
    \include{tables/executionSpeedTable}
    \caption{Execution time for the equation solver for different field sizes $N$ and cores $C$}
    \label{tab:SpeedupSimulation}
\end{table}

\begin{algorithm}[p]
    \caption{Simulate}
    \begin{algorithmic}
        \STATE Setup the equation system
        \STATE Solve the equation system with the conjugate gradient method
        \STATE Calculate the stress using the Van-Mieses-Equation
    \end{algorithmic}
    \label{alg:Simulation}
\end{algorithm}

The reason for using \texttt{OpenMP} is the tight coupling between the data in the sparse matrix multiplication. \texttt{gprof} revealed that the equation solver spends most of it's time in the sparse matrix multiplication, however the whole equation solver class works parallelized. The concept of the simulation is visualized in algorithm~\ref{alg:Simulation}.

\begin{itemize}
    \item \textbf{Threads are spawned at the highest level} of the \texttt{PerformanceEvaluator} class. Subsequent calls to the equation setup, equation solver or linear algrabra operations will not spawn new threads. They require the threads to be set up already, and process only their their chunk using the \texttt{omp for} directive. All for loops are executed in chunks to \textbf{prevent false sharing}.
    \item The \textbf{equation setup} process adds the local element stiffness matrix into the global equation system. Depending on the mechanical structure, the position of different planes appears rather random. Therefore, a lock is needed to prevent a race condition. Because the matrix is sparse and each thread works on its own plane, that typically are not directly connected, it is unprobable that two threads operate on the same equation row at the same time. A global lock would introduce an unnecessary penalty, therefore each row uses it's own \textbf{lock}.
    \item \textbf{All linear algebra operations of the equation solver work in parallel}, we can divide these in two types:
    
    For addition, scalar multiplication, matrix multiplication or assigning a constant value, each thread processes its \textbf{own chunk of rows}. These operations do not require an implicit or explicit barrier. If for example, a vector addition follows a scalar multiplication, it is fine if the first thread begins with the scalar multiplication before the second thread has finished the vector addition, since each of them operates on its own set of rows.
    
    However, in the conjugate gradient method there are also operations like the scalar product or the norm of the vector. These operations require a \textbf{reduction} and have an implicit barrier.
    \item In the beginning, a unique index has to be assigned to each Plane and corner. This is not parallelizable, as the total number of planes and corners is unkown and does not follow a predictable pattern.
    \item Using the \textbf{conjuage gradient method} reduced the time complexity by $O(N)$~\cite{ProgressReport}.
    \item Using a \textbf{sparse matrix multiplication} reduced the the time complexity by $O(N^2)$~\cite{ProgressReport}. 
\end{itemize}

The time complexity of the sequential equation solver is dominated by the sparse matrix multiplication, having a complexity of 

\begin{equation}
    O_\mathit{seqSolve}(N) = O(N^3) 
\end{equation}

as explained in the progress report~\cite{ProgressReport}. The sparse matrix multiplication has been fully parallelized. The indexing of the equation requires $O(N^2)$. Leaving the overhead for thread creation aside, the runtime complexity is 

\begin{equation}
    O_\mathit{parSolve}(N, C) = O \left(\frac{N^3}{C}\right) + O(N^2)
\end{equation}

for a sufficiently large $N$. The efficency is

\begin{equation}
    E = \frac{O_\mathit{seqSolve}}{O_\mathit{parSolve} C} = O\left(\frac{N^3}{N^3 + C^2}\right)
\end{equation}

As the efficency remains only if $N \propto C$, we can asses that the equation solver has Weak Scalibality. However, there is a significant overhead due to creating the threads. The speedup becomes only notable for problem sizes $N > 100$.

The Equation Solver is still a powerful solver. It is able to solve a mechanical structure with 80600 equations in \SI{2.732}{\second} using 28 cores on bridges. It also showcases OpenMP and locks well. The speedup of the execution time of table~\ref{fig:SpeedupSimulation} are shown in figure~\ref{tab:SpeedupSimulation}.

\subsection{Speeding up the evolutionary algorithm}

Evolutionary algoirthms are easily parallelizable by nature. Each organism can be evaluated independently by each node, while the results will be gathered by a single node in charge of sorting them. The best 10\% get broadcasted to the other nodes again. This is visualized in algorithm~\ref{alg:Evolution}.

A MPI Message for an evaluated board consisits of

\begin{itemize}
    \item 1 float for the score
    \item $R \times C$ bytes, each containing the boolean value of the field.
\end{itemize}

We typically simulate grid sizes of 20, meaning the message has a size of 404 bytes. Since all processes will be initialized with the same field size it has not to be stored in the message. The message size grows with 

\begin{equation}
    O_{msgSize} = O(N^2)
\end{equation}

The runtime complexity of the single thread evolutionary algorithm is

\begin{equation}
    O_\mathit{evolSeq}(N, G, A) = O_\textit{seqSolve} \cdot O(G A)
\end{equation}

with generations size $G$ and $A$ epochs. The parallel version has

\begin{equation}
    O_\mathit{evolPar}(N, C, G, A) = O \left( \frac{N^3 G A}{C} + N^2 G A\right)
\end{equation}

with $O (\frac{N^3 G A}{C} )$ being the work for solving the equation system per core and $O(N^2 G A)$ being the communcation cost for sending $O(G)$ bytes of data over the network each epoch~$A$. The efficency is

\begin{equation}
    E = O \left(\frac{N^3 + N^2}{N^3+C N^2}\right)
\end{equation}

so this algorithm has also a weak scalability. Table~\ref{tab:SpeedupEvolution} shows the execution times of the program. They are visualized in figure~\ref{fig:SpeedupEvolution}. The algorihm scales well for small core counts $C < 112$. Each generation has 112 organisims, so up to this part, the evolution scales very well. For $C > 112$, the mechanical solver starts to parallelize. This gives a speedup only if the problem size $N > 100$ is sufficient. The actual execution time starts to reach into hours for that problem size, but this is a typical for a evolutionary algorithm. The result of the finished evolution is shown on figure~\ref{fig:Evolution}.

\medskip

\bibliographystyle{unsrt}
\bibliography{sources}

\begin{algorithm}[p]
    \caption{Evolute on node}
    \begin{algorithmic}
        \STATE Initialize all local fields fully set
        \FOR{all epochs}
            \STATE Mutate the fields
            \STATE Evaluate the fields
            \STATE \texttt{Gather} all fields into the master
            \STATE Sort the fields on the master
            \STATE \texttt{Broadcast} the best 10\% to everyone
            \STATE Replace local fields
        \ENDFOR
    \end{algorithmic}
    \label{alg:Evolution}
\end{algorithm}

\begin{table}[p]
    \centering
    \begin{tabular}{lrrrrrr}
        \toprule
        Cores & 7 & 14 & 28 & 56 & 112 & 224 \\
        Cores per Task & 1 & 1 & 1 & 1 & 1 & 2 \\
        Tasks & 7 & 14 & 28 & 56 & 112 & 112 \\
        Nodes & 1 & 1 & 1 & 2 & 4 & 8 \\
        \midrule
        N = 10 & 33.667     & 18.519 & 9,399 & 5.432 & 2.950 & 4.256 \\
        N = 20 & 296.997    & 164.875 & 91.958 & 43.114 & 25.199 & 29.429 \\
        N = 40 & *2029.938 & *1073.118 & *631.448 & 335.852 & 200.529 & 164.243 \\
        N = 80 & - & *19588.620 & *14045.210 & *7105.495 & *3729.680 & 2163.535 \\
        \bottomrule
    \end{tabular}
    \caption{Execution time in seconds for the evoultion for different field sizes $N$ and cores $C$. The generation size is set fixed to 112 and 1000 epochs are simulated. Values marked with * are extrapolated from a run with less epochs.}
    \label{tab:SpeedupEvolution}
\end{table}

\begin{figure}[p]
    \centering
    \include{graphs/evolutionResults}
    \caption{Speedup of the equation solver for different proplem sizes $N$.}
    \label{fig:SpeedupEvolution}
\end{figure}


\clearpage

\begin{figure}[p]
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug0.png} 
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug900.png} 
    \end{subfigure}

    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug1500.png} 
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/debug8625.png} 
    \end{subfigure}
    \caption{Result of a evoultion, using $20\times20$ gird, 100 organisms per epoch, 1000 epochs and a alterations decay of 0.995. Note, how the structure gets wider on the top to deal with the increased bending stress.}
    \label{fig:Evolution}
\end{figure}


\end{document}